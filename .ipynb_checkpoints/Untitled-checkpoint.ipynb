{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff45b7c1-0be1-4aaf-be67-8b0e2cfb1267",
   "metadata": {},
   "source": [
    "## **Eduardo Carrasco Vidal** <img src=\"img/logo.png\" align=\"right\" style=\"width: 120px;\"/>\n",
    "\n",
    "**Magister en Inteligencia Artificial, Universidad Adolfo Ibáñez.**\n",
    "\n",
    "**Profesor:** Jorge Vásquez.\n",
    "**Curso:** Aprendizaje Reforzado (Reinforcement Learning).\n",
    "\n",
    "Enlace al repositorio del alumno en [GitHub](https://github.com/educarrascov/MIA_ReinforcementLearning/blob/main/Untitled.ipynb) _@educarrascov_\n",
    "\n",
    "![Python](https://img.shields.io/badge/python-%2314354C.svg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4953c337-e730-4b8a-a01b-f2a9dff5ecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gym stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9797d587-03a8-4333-9749-2e6050a3ecf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a5e34f00-ee09-4cbc-989e-906225e9cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyglet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1bc98b-c863-430e-99af-7529e1b6b59a",
   "metadata": {},
   "source": [
    "# CartPole-V0:\n",
    "\n",
    "Un poste es atado a un carro, el cual se mueve a lo largo de un track sin fricción. El péndulo se ubica sobre el carro y el objetivo es balancear el poste aplicando una fuerza en la izquierda o derecha sobre el carro.\n",
    "Una recompensa de más 1 se otorga en cada timestep en que el pendulo permanezca erguido, el juego termina cuando el péndulo tenga una diferencia de más de 15 grados con respecto a la vertical o el carro se mueva más de 2.4 unidades desde el centro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7d1a42-73b1-4dc1-972d-27fccb7d672f",
   "metadata": {},
   "source": [
    "## I. Espacio de Acciones y Estados:\n",
    "\n",
    "- Los Entornos vienen con las variables `state_space` y `action_space`. \n",
    "- `state_space` también lo llaman `observation_space`.\n",
    "- `state` es la información que le da el entorno al agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a0747def-628a-468b-944b-c80bf39efc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import numpy as np \n",
    "import gym\n",
    "import modules\n",
    "import time, math, random\n",
    "from typing import Tuple\n",
    "import pyglet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f70ede-372d-434d-b5bc-d19840af812c",
   "metadata": {},
   "source": [
    "## 1. Generación del ambiente y descripción:\n",
    "En este caso, de los entornos disponibles, seleccionamos el CartPole-v0, _dará una alarma de que está pasado de moda y que se debe usar el v1 mejor, pero está bien_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "41abce26-38fc-45c1-8495-0cea19642701",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v0'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b91bb10-d620-437b-b2fc-d84ca4669900",
   "metadata": {},
   "source": [
    "En el siguiente comando observamos las dimensiones del espacio de acciones o `action_space`. Donde de acuerdo a la descripción del problema, sabemos que son 2 dimensiones con **número 0 que simboliza empujar el carro a la izquierda y con 1 que simboliza empujar el carro a la derecha**.\n",
    "\n",
    "| Num | Action                 |\n",
    "|-----|------------------------|\n",
    "| 0   | Push cart to the left  |\n",
    "| 1   | Push cart to the right |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9894ae11-d14d-444c-97dc-a4e4efc31c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space # ver la dimensión del espacio de acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e448e53c-e791-4fa7-a5c6-e24a29ead18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample() #una acción aleatoria entre las 2 posibles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dbd99e-c533-42b9-8354-4e6b3b1a1e48",
   "metadata": {},
   "source": [
    "En el siguiente comando observamos las dimensiones del espacio de estados o `observation_space`. Donde de acuerdo a la descripción del problema, sabemos que son 4 dimensiones, que pueden tomar los valores que aparecen en la siguiente tabla:\n",
    "\n",
    "| Num | Observation           | Min                 | Max               |\n",
    "|-----|-----------------------|---------------------|-------------------|\n",
    "| 0   | Cart Position         | -4.8                | 4.8               |\n",
    "| 1   | Cart Velocity         | -Inf                | Inf               |\n",
    "| 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
    "| 3   | Pole Angular Velocity | -Inf                | Inf               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9756f885-5132-4a7f-b501-87c16cdd4078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space # ver la dimensión y el vector actual del espacio de estados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4e19de92-d727-48e6-853f-8926abe97964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.5471907e+00, -3.1000888e+38, -1.3440247e-01, -1.1388717e+38],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample() #un vector de estados aleatorio que se encuentre dentro de los valores asociados a cada dimensión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf7d8c4-f25e-4460-bcc4-5bf3f1c1aa84",
   "metadata": {},
   "source": [
    "Para conocer el estado inicial, se aplica el siguiente comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "12b4351f-d653-4096-b9cf-a7b329c2f349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02307805, -0.03951772,  0.03285949, -0.00606013], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset() # Resetear el entorno a su estado inicial, antes de cada episodio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e816dd9f-c4e6-41e6-ac65-d50e8b9664f7",
   "metadata": {},
   "source": [
    "## 2. Visualización del Ambiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6bd2b872-d159-455b-ac1e-d40b7e6988e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = lambda obs: 1 #en este caso, seleccionamos 1 por lo cual, el movimiento del carro será a la derecha\n",
    "\n",
    "for _ in range(3):\n",
    "    obs = env.reset() #Resetear el entorno a su estado inicial\n",
    "    for _ in range(80):\n",
    "        actions = policy(obs) # escoger la acción, vector que incluye action_space\n",
    "        obs, reward, done, info = env.step(actions)\n",
    "        env.render()\n",
    "        time.sleep(0.05)\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5db370-8f4a-4b92-830a-7692c78e5723",
   "metadata": {},
   "source": [
    "Como se observa en el cuadro, el carro se mueve a la derecha constantemente ocasionando que el poste con el péndulo caigan con un giro hacia la izquierda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e7f61-a7d9-41dd-aa9d-2fbbcae47b78",
   "metadata": {},
   "source": [
    "Para observar el detalle del ambiente seleccionado, podemos usar el siguiente comando que indica los siguientes parámetros:\n",
    "- Description.\n",
    "- Source.\n",
    "- `observation_space`.\n",
    "- `action_space`.\n",
    "- Reward.\n",
    "- Starting Space.\n",
    "- Episode Termination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "13074294-1f68-40b5-8968-2ec47a32ffd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        CartPoleEnv\n",
       "\u001b[0;31mString form:\u001b[0m <CartPoleEnv<CartPole-v0>>\n",
       "\u001b[0;31mFile:\u001b[0m        ~/opt/anaconda3/lib/python3.9/site-packages/gym/envs/classic_control/cartpole.py\n",
       "\u001b[0;31mDocstring:\u001b[0m  \n",
       "Description:\n",
       "    A pole is attached by an un-actuated joint to a cart, which moves along\n",
       "    a frictionless track. The pendulum starts upright, and the goal is to\n",
       "    prevent it from falling over by increasing and reducing the cart's\n",
       "    velocity.\n",
       "\n",
       "Source:\n",
       "    This environment corresponds to the version of the cart-pole problem\n",
       "    described by Barto, Sutton, and Anderson\n",
       "\n",
       "Observation:\n",
       "    Type: Box(4)\n",
       "    Num     Observation               Min                     Max\n",
       "    0       Cart Position             -4.8                    4.8\n",
       "    1       Cart Velocity             -Inf                    Inf\n",
       "    2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
       "    3       Pole Angular Velocity     -Inf                    Inf\n",
       "\n",
       "Actions:\n",
       "    Type: Discrete(2)\n",
       "    Num   Action\n",
       "    0     Push cart to the left\n",
       "    1     Push cart to the right\n",
       "\n",
       "    Note: The amount the velocity that is reduced or increased is not\n",
       "    fixed; it depends on the angle the pole is pointing. This is because\n",
       "    the center of gravity of the pole increases the amount of energy needed\n",
       "    to move the cart underneath it\n",
       "\n",
       "Reward:\n",
       "    Reward is 1 for every step taken, including the termination step\n",
       "\n",
       "Starting State:\n",
       "    All observations are assigned a uniform random value in [-0.05..0.05]\n",
       "\n",
       "Episode Termination:\n",
       "    Pole Angle is more than 12 degrees.\n",
       "    Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
       "    the display).\n",
       "    Episode length is greater than 200.\n",
       "    Solved Requirements:\n",
       "    Considered solved when the average return is greater than or equal to\n",
       "    195.0 over 100 consecutive trials.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?env.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1be206a8-4ebc-4e6b-9747-35565c7760a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.474899 , 14.568684 , -5.7374268, -1.1956105], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs #esta variable, definida en la función anterior, demuestra de igual manera el estado inicial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb708601-f245-425b-8428-4b7245f5edb5",
   "metadata": {},
   "source": [
    "## 3. Implementación del Q-Learning:\n",
    "_El Q-learning es un algoritmo de aprendizaje basado en valores y se centra en la optimización de la función de valor según el entorno o el problema. La Q en el Q-learning representa la calidad con la que el modelo encuentra su próxima acción mejorando la calidad_ \n",
    "Puede ser aplicado a cualquier estado finito de MDP (**\"Markov Decision Process\"**). [Acceso a referencia](https://datascience.eu/es/aprendizaje-automatico/q-learning/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02311c5c-0358-461b-8b59-c212ee536229",
   "metadata": {},
   "source": [
    "Lo primero que se debe hacer es convertir las acciones continuas en discretas, efectuando una división del espacio de estados en bins, en este caso las variables las dividimos en 6 y 12 bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b20a0123-928d-415a-997e-a089718e40b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = ( 6 , 12 )\n",
    "lower_bounds = [ env.observation_space.low[2], -math.radians(50) ]\n",
    "upper_bounds = [ env.observation_space.high[2], math.radians(50) ]\n",
    "\n",
    "def discretizer( _ , __ , angle, pole_velocity ) -> Tuple[int,...]:\n",
    "    \"\"\"Convert continues state intro a discrete state\"\"\"\n",
    "    est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "    est.fit([lower_bounds, upper_bounds ])\n",
    "    return tuple(map(int,est.transform([[angle, pole_velocity]])[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdb5eed-bc1c-4d3d-b0fc-16e834567dd2",
   "metadata": {},
   "source": [
    "### 1.1. Inicialización:\n",
    "Acá inicializamos los valores de la tabla Q, con ceros. \n",
    "_El agente al jugar el juego por primera vez no incluirá ningún conocimiento. Por lo tanto, asumiremos que la tabla Q es cero_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3151250f-aa9f-49fd-ad28-5eaf3f16e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros(n_bins + (env.action_space.n,))\n",
    "Q_table.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573d0707-419c-4efc-9b0f-3819afb4b5d6",
   "metadata": {},
   "source": [
    "### 1.2. Exploración o Aprendizaje:\n",
    "El agente elegirá cualquiera de los dos caminos posibles. \n",
    "\n",
    "Si el agente **Aprende**, recogerá información de la tabla Q, o cuando el agente **explora**, intentará hacer nuevos caminos.\n",
    "\n",
    "- Cuando el agente trabaja para un número mayor durante un tiempo, es esencial **aprender**.\n",
    "- Cuando su agente no tiene ninguna experiencia, es esencial **explorar**.\n",
    "\n",
    "Definimos un policy, que use la tabla Q (nos indicará el valor de la política para un estado y una acción determinados) y que seleccione la con mayor valor en un estado dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd312381-f415-4d6b-b223-302b793f2809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy( state : tuple ):\n",
    "    \"\"\"Choosing action based on epsilon-greedy policy\"\"\" # se dejará llevar por grandes recompensas inmediatas.\n",
    "    return np.argmax(Q_table[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3c8a6b-4773-40a8-ad01-827eb6f06f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Actualización de la función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8a0d93-41c6-4086-acd4-3d1368bdd2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_Q_value( reward : float ,  new_state : tuple , discount_factor=1 ) -> float:\n",
    "    \"\"\"Temperal diffrence for updating Q-value of state-action pair\"\"\"\n",
    "    future_optimal_value = np.max(Q_table[new_state])\n",
    "    learned_value = reward + discount_factor * future_optimal_value\n",
    "    return learned_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cda0ac2-59eb-4c58-8fce-f98692a1962c",
   "metadata": {},
   "source": [
    "#### 1.2.1. Aprendizaje:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7739770-abca-4ed9-837b-cc878c31107a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive learning of Learning Rate\n",
    "def learning_rate(n : int , min_rate=0.01 ) -> float  :\n",
    "    \"\"\"Decaying learning rate\"\"\"\n",
    "    return max(min_rate, min(1.0, 1.0 - math.log10((n + 1) / 25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f67054-1ea7-4f69-942a-0ee93dbeb7e3",
   "metadata": {},
   "source": [
    "#### 1.2.2. Exploración:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4326ff-eab9-4703-aca2-2dce34bc4340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploration_rate(n : int, min_rate= 0.1 ) -> float :\n",
    "    \"\"\"Decaying exploration rate\"\"\"\n",
    "    return max(min_rate, min(1, 1.0 - math.log10((n  + 1) / 25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7ec695-4406-4fba-a4be-be754b5a7f1d",
   "metadata": {},
   "source": [
    "## 4. Entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f6404e-e780-4b35-b5af-269ecae7741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 10000 \n",
    "for e in range(n_episodes):\n",
    "    \n",
    "    # Siscretize state into buckets\n",
    "    current_state, done = discretizer(*env.reset()), False\n",
    "    \n",
    "    while done==False:\n",
    "        \n",
    "        # policy action \n",
    "        action = policy(current_state) # exploit\n",
    "        \n",
    "        # insert random action\n",
    "        if np.random.random() < exploration_rate(e) : \n",
    "            action = env.action_space.sample() # explore \n",
    "         \n",
    "        # increment enviroment\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        new_state = discretizer(*obs)\n",
    "        \n",
    "        # Update Q-Table\n",
    "        lr = learning_rate(e)\n",
    "        learnt_value = new_Q_value(reward , new_state )\n",
    "        old_value = Q_table[current_state][action]\n",
    "        Q_table[current_state][action] = (1-lr)*old_value + lr*learnt_value\n",
    "        \n",
    "        current_state = new_state\n",
    "        \n",
    "        # Render the cartpole environment\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d84167-76be-4eab-8787-ac4fac74e954",
   "metadata": {},
   "source": [
    "## 5. Referencias:\n",
    "- [1]. https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\n",
    "- [2]. https://www.gymlibrary.ml\n",
    "- [3]. https://towardsdatascience.com/getting-started-with-openai-gym-d2ac911f5cbc\n",
    "- [4]. https://medium.com/analytics-vidhya/q-learning-is-the-most-basic-form-of-reinforcement-learning-which-doesnt-take-advantage-of-any-8944e02570c5\n",
    "- [5]. https://github.com/JackFurby/CartPole-v0\n",
    "- [6]. https://github.com/RJBrooker/Q-learning-demo-Cartpole-V1/blob/master/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d26200-5657-4813-af8e-e5df3ffa3f88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
